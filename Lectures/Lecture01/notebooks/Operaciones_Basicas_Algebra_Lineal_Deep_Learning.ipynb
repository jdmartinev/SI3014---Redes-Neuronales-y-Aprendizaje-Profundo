{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operaciones Básicas de Álgebra Lineal en Deep Learning\n",
    "\n",
    "**Instructores:** \n",
    "- Ph.D Juan David Martínez Vargas\n",
    "- PhD. Raúl Andrés Castañeda Quintero\n",
    "\n",
    "**Institución:** Escuela de Ciencias Aplicadas e Ingeniería - Universidad EAFIT\n",
    "\n",
    "---\n",
    "\n",
    "## Contenido\n",
    "\n",
    "1. Tensores en el aprendizaje profundo\n",
    "2. Tensores y PyTorch\n",
    "3. Vectores, matrices y broadcasting\n",
    "4. Convenciones de notación para redes neuronales\n",
    "5. Una capa totalmente conectada (lineal) en PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivación: El Problema XOR\n",
    "\n",
    "<!-- INSERT FIGURE: Page 2 - Neural network diagram showing MLP with one hidden layer -->\n",
    "\n",
    "Para resolver el problema XOR necesitamos un **MLP (Multi-Layer Perceptron) con una capa oculta** y función de activación no lineal (ReLU). Este es un ejemplo clásico que muestra por qué necesitamos redes neuronales multicapa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Conceptos Fundamentales\n",
    "\n",
    "### 1.1 Escalares\n",
    "\n",
    "Un **escalar** es un número real que representa una magnitud sin dirección. A diferencia de los vectores y matrices, un escalar no tiene dimensiones ni orientación.\n",
    "\n",
    "**Notación matemática:**\n",
    "$$c \\in \\mathbb{R}$$\n",
    "\n",
    "**En Deep Learning, los escalares se utilizan para:**\n",
    "- Ajustar pesos\n",
    "- Medir errores (función de pérdida)\n",
    "- Controlar el aprendizaje (learning rate)\n",
    "- Representar salidas numéricas (regresión)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de escalares en Python\n",
    "learning_rate = 0.01\n",
    "loss = 0.5\n",
    "weight = 1.5\n",
    "\n",
    "print(f\"Learning rate: {learning_rate}\")\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Weight: {weight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Vectores\n",
    "\n",
    "Un **vector** es un objeto matemático que representa una colección ordenada de números, los cuales pueden interpretarse como componentes de magnitud y dirección en un espacio.\n",
    "\n",
    "**Notación matemática:**\n",
    "$$\\vec{b} = (b_1, b_2, b_3, ..., b_n)$$\n",
    "\n",
    "**En Deep Learning, un vector representa información estructurada:**\n",
    "- Entrada de una red neuronal\n",
    "- Pesos de una neurona\n",
    "- Salida de una capa\n",
    "- Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Crear un vector\n",
    "vector = np.array([1, 2, 3, 4, 5])\n",
    "print(f\"Vector: {vector}\")\n",
    "print(f\"Dimensión: {vector.shape}\")\n",
    "print(f\"Tipo: {type(vector)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Suma de Vectores\n",
    "\n",
    "La suma de vectores es una operación que combina dos vectores de igual dimensión sumando sus componentes correspondientes.\n",
    "\n",
    "<!-- INSERT FIGURE: Page 4 - Vector addition diagram -->\n",
    "\n",
    "**En Deep Learning se utiliza para:**\n",
    "- Añadir el sesgo (bias) en una neurona\n",
    "- Combinar activaciones\n",
    "- Implementar conexiones residuales\n",
    "\n",
    "**Notación matemática:**\n",
    "$$\\vec{R} = \\vec{a} + \\vec{b}$$\n",
    "\n",
    "**En 2D:**\n",
    "$$\\vec{R} = (a_x + b_x)\\hat{i} + (a_y + b_y)\\hat{j}$$\n",
    "\n",
    "**Magnitud:**\n",
    "$$|\\vec{R}| = \\sqrt{R_x^2 + R_y^2}$$\n",
    "\n",
    "**Dirección:**\n",
    "$$\\theta = \\tan^{-1}\\left(\\frac{R_y}{R_x}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suma de vectores\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "\n",
    "resultado = a + b\n",
    "print(f\"a = {a}\")\n",
    "print(f\"b = {b}\")\n",
    "print(f\"a + b = {resultado}\")\n",
    "\n",
    "# Propiedades\n",
    "print(f\"\\nPropiedades:\")\n",
    "print(f\"Conmutativa: a + b = {a + b}, b + a = {b + a}\")\n",
    "print(f\"¿Son iguales? {np.array_equal(a + b, b + a)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Multiplicación Escalar por Vector\n",
    "\n",
    "La multiplicación de un escalar por un vector consiste en multiplicar cada componente del vector por un mismo número real.\n",
    "\n",
    "**Efectos según el valor del escalar $c$:**\n",
    "\n",
    "- Si $0 < c < 1$: la magnitud disminuye\n",
    "- Si $c > 1$: la magnitud aumenta\n",
    "- Si $-1 < c < 0$: la dirección cambia 180° y la magnitud disminuye\n",
    "- Si $c < -1$: la dirección cambia 180° y la magnitud aumenta\n",
    "- Si $c = 0$: el vector resultado es nulo\n",
    "\n",
    "**En Deep Learning se utiliza para:**\n",
    "- Ajustar la contribución de características\n",
    "- Pesos y activaciones: escalan las entradas antes de combinarlas\n",
    "- Learning rate: $\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta\\nabla L$\n",
    "- Normalización y estabilidad numérica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplicación escalar por vector\n",
    "vector = np.array([1, 2, 3])\n",
    "\n",
    "# Diferentes escalares\n",
    "escalares = [0.5, 2, -1, -2, 0]\n",
    "\n",
    "for c in escalares:\n",
    "    resultado = c * vector\n",
    "    print(f\"{c} * {vector} = {resultado}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Producto Punto (Dot Product)\n",
    "\n",
    "El producto punto es una operación fundamental que resulta en un escalar. Se considera la proyección de un vector sobre otro.\n",
    "\n",
    "<!-- INSERT FIGURE: Page 6 - Dot product geometric interpretation -->\n",
    "\n",
    "**Notación:** $\\vec{a} \\cdot \\vec{b}$\n",
    "\n",
    "**Dos formas de calcularlo:**\n",
    "\n",
    "1. **Forma geométrica:**\n",
    "$$\\vec{a} \\cdot \\vec{b} = |\\vec{a}||\\vec{b}|\\cos\\theta$$\n",
    "\n",
    "2. **Forma algebraica:**\n",
    "$$\\vec{x} \\cdot \\vec{w} = \\sum_{i=1}^{n} w_i x_i = w_1x_1 + w_2x_2 + \\cdots + w_nx_n$$\n",
    "\n",
    "**Propiedades:**\n",
    "- Conmutativa: $\\vec{a} \\cdot \\vec{b} = \\vec{b} \\cdot \\vec{a}$\n",
    "- Distributiva: $\\vec{a} \\cdot (\\vec{b} + \\vec{c}) = \\vec{a} \\cdot \\vec{b} + \\vec{a} \\cdot \\vec{c}$\n",
    "- Asociativa (escalar): $p(\\vec{b} \\cdot \\vec{c}) = (p\\vec{a}) \\cdot \\vec{b}$\n",
    "- $\\vec{a} \\cdot \\vec{0} = 0$\n",
    "- $\\vec{a} \\cdot \\vec{a} = |\\vec{a}|^2$\n",
    "\n",
    "**En Deep Learning:**\n",
    "El producto punto se usa para combinar entradas con pesos y medir similitud entre representaciones, siendo la operación central de neuronas, atención y convoluciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Producto punto en NumPy\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "\n",
    "# Método 1: usando .dot()\n",
    "producto_1 = a.dot(b)\n",
    "print(f\"Usando .dot(): {producto_1}\")\n",
    "\n",
    "# Método 2: usando np.dot()\n",
    "producto_2 = np.dot(a, b)\n",
    "print(f\"Usando np.dot(): {producto_2}\")\n",
    "\n",
    "# Método 3: manual\n",
    "producto_3 = sum(a * b)\n",
    "print(f\"Manual: {producto_3}\")\n",
    "\n",
    "# Todos deberían dar el mismo resultado\n",
    "print(f\"\\nTodos iguales: {producto_1 == producto_2 == producto_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo: Producto Punto (Forma Geométrica vs Algebraica)\n",
    "\n",
    "<!-- INSERT FIGURE: Page 7 - Vector diagram showing angle calculation -->\n",
    "\n",
    "Considere los vectores:\n",
    "- $\\vec{a} = -5\\hat{i} + 4\\hat{j}$\n",
    "- $\\vec{b} = -3\\hat{i} - 8\\hat{j}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir vectores\n",
    "a = np.array([-5, 4])\n",
    "b = np.array([-3, -8])\n",
    "\n",
    "# Método 1: Algebraico\n",
    "producto_algebraico = np.dot(a, b)\n",
    "print(f\"Método algebraico: {producto_algebraico}\")\n",
    "\n",
    "# Método 2: Geométrico\n",
    "# Calcular magnitudes\n",
    "mag_a = np.linalg.norm(a)\n",
    "mag_b = np.linalg.norm(b)\n",
    "\n",
    "# Calcular ángulos\n",
    "angulo_a = np.degrees(np.arctan2(a[1], a[0]))\n",
    "angulo_b = np.degrees(np.arctan2(b[1], b[0]))\n",
    "theta = angulo_a - angulo_b\n",
    "\n",
    "# Producto usando forma geométrica\n",
    "producto_geometrico = mag_a * mag_b * np.cos(np.radians(theta))\n",
    "\n",
    "print(f\"\\nMagnitud de a: {mag_a:.2f}\")\n",
    "print(f\"Magnitud de b: {mag_b:.2f}\")\n",
    "print(f\"Ángulo α: {angulo_a:.2f}°\")\n",
    "print(f\"Ángulo β: {angulo_b:.2f}°\")\n",
    "print(f\"Ángulo θ entre vectores: {theta:.2f}°\")\n",
    "print(f\"\\nProducto geométrico: {producto_geometrico:.2f}\")\n",
    "print(f\"\\nAmbos métodos coinciden: {np.isclose(producto_algebraico, producto_geometrico)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Matrices\n",
    "\n",
    "Una **matriz** es un arreglo bidimensional de números organizado en filas y columnas. Representa relaciones entre múltiples variables y permite describir transformaciones lineales y sistemas de datos estructurados.\n",
    "\n",
    "**Notación:**\n",
    "$$\\mathbf{W} = \\begin{bmatrix}\n",
    "w_{11} & w_{12} & \\cdots & w_{1n} \\\\\n",
    "w_{21} & w_{22} & \\cdots & w_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{m1} & w_{m2} & \\cdots & w_{mn}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "**En Deep Learning, las matrices se utilizan para:**\n",
    "- Representar pesos de una capa neuronal\n",
    "- Modelar transformaciones lineales entre capas\n",
    "- Implementar convoluciones y operaciones lineales\n",
    "- Calcular atención y relaciones entre múltiples vectores\n",
    "\n",
    "<!-- INSERT FIGURE: Page 8 - Image showing matrix representation of image pixels -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear matrices\n",
    "matriz_2x3 = np.array([[1, 2, 3],\n",
    "                       [4, 5, 6]])\n",
    "\n",
    "print(\"Matriz 2x3:\")\n",
    "print(matriz_2x3)\n",
    "print(f\"Forma: {matriz_2x3.shape}\")\n",
    "print(f\"Número de dimensiones: {matriz_2x3.ndim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Multiplicación de Matrices\n",
    "\n",
    "<!-- INSERT FIGURE: Page 9 - Matrix multiplication step by step diagram -->\n",
    "\n",
    "La multiplicación de matrices $\\mathbf{C} = \\mathbf{A} \\cdot \\mathbf{B}$ se realiza calculando el producto punto de cada fila de $\\mathbf{A}$ con cada columna de $\\mathbf{B}$.\n",
    "\n",
    "Para una matriz 2×2:\n",
    "\n",
    "$$\\mathbf{C} = \\begin{bmatrix}\n",
    "A_{11} & A_{12} \\\\\n",
    "A_{21} & A_{22}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "B_{11} & B_{12} \\\\\n",
    "B_{21} & B_{22}\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "C_{11} & C_{12} \\\\\n",
    "C_{21} & C_{22}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Donde:\n",
    "- $C_{11} = A_{11}B_{11} + A_{12}B_{21}$\n",
    "- $C_{12} = A_{11}B_{12} + A_{12}B_{22}$\n",
    "- $C_{21} = A_{21}B_{11} + A_{22}B_{21}$\n",
    "- $C_{22} = A_{21}B_{12} + A_{22}B_{22}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplicación de matrices\n",
    "A = np.array([[1, 2],\n",
    "              [3, 4]])\n",
    "\n",
    "B = np.array([[5, 6],\n",
    "              [7, 8]])\n",
    "\n",
    "# Método 1: usando @\n",
    "C1 = A @ B\n",
    "\n",
    "# Método 2: usando np.matmul\n",
    "C2 = np.matmul(A, B)\n",
    "\n",
    "# Método 3: usando .dot()\n",
    "C3 = A.dot(B)\n",
    "\n",
    "print(\"Matriz A:\")\n",
    "print(A)\n",
    "print(\"\\nMatriz B:\")\n",
    "print(B)\n",
    "print(\"\\nProducto A @ B:\")\n",
    "print(C1)\n",
    "\n",
    "# Verificar que todos dan el mismo resultado\n",
    "print(f\"\\nTodos los métodos coinciden: {np.array_equal(C1, C2) and np.array_equal(C2, C3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Cálculo con Múltiples Ejemplos de Entrenamiento\n",
    "\n",
    "Cuando tenemos $n$ ejemplos de entrenamiento, podemos procesarlos todos a la vez usando multiplicación de matrices:\n",
    "\n",
    "$$\\mathbf{X}\\mathbf{w} + b = \\mathbf{z}$$\n",
    "\n",
    "Donde:\n",
    "- $\\mathbf{X} \\in \\mathbb{R}^{n \\times m}$ (n ejemplos, m características)\n",
    "- $\\mathbf{w} \\in \\mathbb{R}^{m \\times 1}$ (pesos)\n",
    "- $\\mathbf{z} \\in \\mathbb{R}^{n \\times 1}$ (salidas)\n",
    "\n",
    "**Dos oportunidades de paralelismo:**\n",
    "1. Multiplicar elementos para calcular el producto punto\n",
    "2. Calcular múltiples productos punto simultáneamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo con múltiples muestras\n",
    "n_samples = 5\n",
    "n_features = 3\n",
    "\n",
    "# Datos de entrada (5 muestras, 3 características)\n",
    "X = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9],\n",
    "              [10, 11, 12],\n",
    "              [13, 14, 15]])\n",
    "\n",
    "# Pesos\n",
    "w = np.array([[0.5],\n",
    "              [0.3],\n",
    "              [0.2]])\n",
    "\n",
    "# Bias\n",
    "b = 1.0\n",
    "\n",
    "# Calcular salidas para todas las muestras a la vez\n",
    "z = X @ w + b\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"w shape: {w.shape}\")\n",
    "print(f\"z shape: {z.shape}\")\n",
    "print(f\"\\nSalidas z:\\n{z}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tensores\n",
    "\n",
    "Un **tensor** es una generalización de los escalares, vectores y matrices a un número arbitrario de dimensiones.\n",
    "\n",
    "<!-- INSERT FIGURE: Page 10 - Tensor hierarchy diagram showing scalar, vector, matrix, tensor -->\n",
    "\n",
    "En Deep Learning no trabajamos con un solo número, ni con un solo vector o una sola matriz, sino con **datos multidimensionales**. Para manejar todo eso de forma unificada usamos tensores.\n",
    "\n",
    "| Objeto | Tensor de orden | Ejemplo |\n",
    "|--------|----------------|----------|\n",
    "| Escalar | Orden 0 | $x \\in \\mathbb{R}$ |\n",
    "| Vector | Orden 1 | $\\mathbf{x} \\in \\mathbb{R}^n$ |\n",
    "| Matriz | Orden 2 | $\\mathbf{W} \\in \\mathbb{R}^{m \\times n}$ |\n",
    "| Tensor | Orden ≥ 3 | $\\mathcal{X} \\in \\mathbb{R}^{n_1 \\times n_2 \\times n_3 \\times \\cdots}$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Introducción a PyTorch\n",
    "\n",
    "PyTorch es una biblioteca de aprendizaje profundo que proporciona:\n",
    "- Arreglos multidimensionales (tensores)\n",
    "- Soporte para GPU\n",
    "- Diferenciación automática\n",
    "- Funciones convenientes para deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar PyTorch (si es necesario)\n",
    "# !pip install torch --break-system-packages\n",
    "\n",
    "import torch\n",
    "\n",
    "print(f\"Versión de PyTorch: {torch.__version__}\")\n",
    "print(f\"¿CUDA disponible? {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Creación de Tensores en PyTorch\n",
    "\n",
    "<!-- INSERT FIGURE: Page 11 - PyTorch tensor creation code example -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un tensor\n",
    "t = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "print(\"Tensor:\")\n",
    "print(t)\n",
    "print(f\"\\nDimensiones (.shape): {t.shape}\")\n",
    "print(f\"Dimensiones (.size()): {t.size()}\")\n",
    "print(f\"Número de dimensiones (.ndim): {t.ndim}\")\n",
    "print(f\"Tipo de dato: {t.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 NumPy vs PyTorch\n",
    "\n",
    "La sintaxis de NumPy y PyTorch es muy similar:\n",
    "\n",
    "| Concepto | NumPy | PyTorch |\n",
    "|----------|-------|----------|\n",
    "| Vector | `np.array` | `torch.tensor` |\n",
    "| Producto punto | `a.dot(b)` | `b.matmul(b)` o `b @ b` |\n",
    "| Resultado | escalar | tensor escalar |\n",
    "| Conversión | — | `b.numpy()` |\n",
    "\n",
    "<!-- INSERT FIGURE: Page 12 & 13 - NumPy and PyTorch comparison code -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparación NumPy vs PyTorch\n",
    "\n",
    "# NumPy\n",
    "a_np = np.array([1., 2., 3.])\n",
    "print(f\"NumPy - Tipo: {type(a_np)}\")\n",
    "print(f\"NumPy - dtype: {a_np.dtype}\")\n",
    "print(f\"NumPy - shape: {a_np.shape}\")\n",
    "\n",
    "# PyTorch\n",
    "b_torch = torch.tensor([1., 2., 3.])\n",
    "print(f\"\\nPyTorch - Tipo: {type(b_torch)}\")\n",
    "print(f\"PyTorch - dtype: {b_torch.dtype}\")\n",
    "print(f\"PyTorch - shape: {b_torch.shape}\")\n",
    "\n",
    "# Conversión\n",
    "b_to_numpy = b_torch.numpy()\n",
    "print(f\"\\nConversión a NumPy: {type(b_to_numpy)}\")\n",
    "print(f\"Convertido - dtype: {b_to_numpy.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Tipos de Datos en PyTorch\n",
    "\n",
    "<!-- INSERT FIGURE: Page 14 - Data types table -->\n",
    "\n",
    "**Tipos importantes:**\n",
    "- `int32/int64`: Enteros (default int en NumPy & PyTorch)\n",
    "- `float32`: Flotante de 32 bits (default float en PyTorch)\n",
    "- `float64`: Flotante de 64 bits (default float en NumPy)\n",
    "\n",
    "**Notas:**\n",
    "- Los flotantes de 32 bits son menos precisos que los de 64 bits\n",
    "- Para redes neuronales, 32 bits es suficiente\n",
    "- Para GPUs regulares, 32 bits es más rápido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tipos de datos\n",
    "\n",
    "# Especificar tipo al crear\n",
    "c_float = torch.tensor([1, 2, 3], dtype=torch.float)\n",
    "print(f\"float: {c_float.dtype}\")\n",
    "\n",
    "c_double = torch.tensor([1, 2, 3], dtype=torch.double)\n",
    "print(f\"double: {c_double.dtype}\")\n",
    "\n",
    "c_float64 = torch.tensor([1, 2, 3], dtype=torch.float64)\n",
    "print(f\"float64: {c_float64.dtype}\")\n",
    "\n",
    "# Convertir tipos\n",
    "d = torch.tensor([1, 2, 3])\n",
    "print(f\"\\nOriginal: {d.dtype}\")\n",
    "\n",
    "e = d.double()\n",
    "print(f\"Convertido a double: {e.dtype}\")\n",
    "\n",
    "f = d.to(torch.float64)\n",
    "print(f\"Convertido con .to(): {f.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 ¿Por qué PyTorch en lugar de NumPy?\n",
    "\n",
    "**PyTorch ofrece:**\n",
    "\n",
    "1. **Soporte para GPU:**\n",
    "   - Cargar datasets y parámetros del modelo en memoria GPU\n",
    "   - Mejor paralelismo para multiplicaciones de matrices\n",
    "\n",
    "2. **Diferenciación automática** (más adelante)\n",
    "\n",
    "3. **Funciones convenientes para deep learning** (más adelante)\n",
    "\n",
    "**PyTorch permite entrenar modelos grandes de forma eficiente usando GPU, calcular derivadas automáticamente y construir redes profundas con herramientas ya implementadas.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Uso de GPU en PyTorch\n",
    "\n",
    "<!-- INSERT FIGURE: Page 18 - GPU code example -->\n",
    "\n",
    "Cargar datos en la GPU es muy fácil en PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar disponibilidad de CUDA\n",
    "print(f\"¿CUDA disponible? {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Mover tensor a GPU\n",
    "    b = torch.tensor([1., 2., 3.])\n",
    "    b_gpu = b.to(torch.device('cuda:0'))\n",
    "    print(f\"Tensor en GPU: {b_gpu}\")\n",
    "    \n",
    "    # Mover de vuelta a CPU\n",
    "    b_cpu = b_gpu.to(torch.device('cpu'))\n",
    "    print(f\"Tensor en CPU: {b_cpu}\")\n",
    "else:\n",
    "    print(\"CUDA no disponible. Usando CPU.\")\n",
    "    # El código seguirá funcionando en CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Verificar Dispositivos CUDA\n",
    "\n",
    "<!-- INSERT FIGURE: Page 19 - nvidia-smi output -->\n",
    "\n",
    "Si tienes CUDA instalado, puedes usar `nvidia-smi` para verificar tus GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Información de GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Número de GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"GPU actual: {torch.cuda.current_device()}\")\n",
    "    print(f\"Nombre de GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # Memoria\n",
    "    print(f\"\\nMemoria asignada: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "    print(f\"Memoria en caché: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"No hay GPU CUDA disponible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Broadcasting en PyTorch\n",
    "\n",
    "Los tensores de PyTorch no son tensores matemáticos \"reales\" - tienen operaciones extendidas que son muy útiles.\n",
    "\n",
    "**Broadcasting** permite operaciones entre tensores de diferentes formas automáticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplos de broadcasting\n",
    "\n",
    "# Escalar + Vector\n",
    "result1 = torch.tensor([1, 2, 3]) + 1\n",
    "print(f\"[1,2,3] + 1 = {result1}\")\n",
    "\n",
    "# Vector * Vector (elemento por elemento)\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "result2 = a * b\n",
    "print(f\"[1,2,3] * [4,5,6] = {result2}\")\n",
    "\n",
    "# Matriz + Vector\n",
    "t = torch.tensor([[4, 5, 6], [7, 8, 9]])\n",
    "result3 = t + torch.tensor([1, 2, 3])\n",
    "print(f\"\\nMatriz + Vector:\\n{result3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- INSERT FIGURE: Page 30 & 31 - Broadcasting visualization -->\n",
    "\n",
    "**Cómo funciona broadcasting:**\n",
    "\n",
    "Se agregan dimensiones implícitas y los elementos se duplican implícitamente para que las formas coincidan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de broadcasting\n",
    "\n",
    "# Caso 1: escalar → vector\n",
    "print(\"Broadcasting escalar a vector:\")\n",
    "print(\"[1, 2, 3] + 1\")\n",
    "print(\"→ [1, 2, 3] + [1, 1, 1]\")\n",
    "print(f\"= {torch.tensor([1, 2, 3]) + 1}\")\n",
    "\n",
    "# Caso 2: vector → matriz\n",
    "print(\"\\nBroadcasting vector a matriz:\")\n",
    "matrix = torch.tensor([[4, 5, 6], [7, 8, 9]])\n",
    "vector = torch.tensor([1, 2, 3])\n",
    "print(\"Matrix shape:\", matrix.shape)\n",
    "print(\"Vector shape:\", vector.shape)\n",
    "result = matrix + vector\n",
    "print(\"Result shape:\", result.shape)\n",
    "print(f\"Resultado:\\n{result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Convenciones de Notación para Redes Neuronales\n",
    "\n",
    "### 6.1 Perceptrón Simple\n",
    "\n",
    "<!-- INSERT FIGURE: Page 33 - Single perceptron diagram -->\n",
    "\n",
    "Para un solo ejemplo de entrenamiento:\n",
    "\n",
    "$$\\mathbf{x}^T\\mathbf{w} + b = z$$\n",
    "\n",
    "Donde:\n",
    "- $\\mathbf{x} = [x_1, x_2, ..., x_m]^T$\n",
    "- $\\mathbf{w} = [w_1, w_2, ..., w_m]^T$\n",
    "- $b$ es el bias\n",
    "- $z$ es la entrada neta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptrón simple\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "w = torch.tensor([0.5, 0.3, 0.2])\n",
    "b = 1.0\n",
    "\n",
    "# Calcular entrada neta\n",
    "z = torch.dot(x, w) + b\n",
    "print(f\"Entrada neta z: {z}\")\n",
    "\n",
    "# Función de activación (por ejemplo, sigmoide)\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + torch.exp(-z))\n",
    "\n",
    "y_pred = sigmoid(z)\n",
    "print(f\"Salida del perceptrón: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Capa Totalmente Conectada\n",
    "\n",
    "<!-- INSERT FIGURE: Page 35 - Fully connected layer diagram -->\n",
    "\n",
    "**Para 1 ejemplo de entrenamiento:**\n",
    "\n",
    "$$\\sigma(\\mathbf{W}\\mathbf{x} + \\mathbf{b}) = \\mathbf{a}$$\n",
    "\n",
    "Donde:\n",
    "- $\\mathbf{a} \\in \\mathbb{R}^{h \\times 1}$ (h neuronas en la capa)\n",
    "- $\\mathbf{W} \\in \\mathbb{R}^{h \\times m}$ (pesos)\n",
    "- $\\mathbf{x} \\in \\mathbb{R}^{m \\times 1}$ (entrada)\n",
    "\n",
    "**Nota importante:** $w_{i,j}$ se refiere al peso que conecta la entrada $j$-ésima con la salida $i$-ésima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Capa con Múltiples Ejemplos\n",
    "\n",
    "<!-- INSERT FIGURE: Page 36 - Multiple examples diagram -->\n",
    "\n",
    "**Para n ejemplos de entrenamiento:**\n",
    "\n",
    "$$\\sigma([\\mathbf{W}\\mathbf{X}^T]^T + \\mathbf{b}) = \\mathbf{A}$$\n",
    "\n",
    "Donde:\n",
    "- $\\mathbf{A} \\in \\mathbb{R}^{n \\times h}$\n",
    "- $\\mathbf{X} \\in \\mathbb{R}^{n \\times m}$ (ejemplos en filas)\n",
    "- $\\mathbf{W}^T \\in \\mathbb{R}^{m \\times h}$\n",
    "\n",
    "**Convención de PyTorch (más conveniente):**\n",
    "\n",
    "$$\\sigma([\\mathbf{X}\\mathbf{W}^T] + \\mathbf{b}) = \\mathbf{A}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Capa Lineal en PyTorch\n",
    "\n",
    "<!-- INSERT FIGURE: Page 42 & 43 - PyTorch Linear layer code -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear datos de entrada (10 ejemplos, 5 características)\n",
    "X = torch.arange(50, dtype=torch.float).view(10, 5)\n",
    "print(\"Datos de entrada X:\")\n",
    "print(X)\n",
    "print(f\"X shape: {X.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una capa lineal (5 entradas → 3 salidas)\n",
    "fc_layer = torch.nn.Linear(in_features=5, out_features=3)\n",
    "\n",
    "print(\"\\nPesos de la capa:\")\n",
    "print(fc_layer.weight)\n",
    "print(f\"W shape: {fc_layer.weight.size()}\")\n",
    "\n",
    "print(\"\\nBias de la capa:\")\n",
    "print(fc_layer.bias)\n",
    "print(f\"b shape: {fc_layer.bias.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar la capa a los datos\n",
    "A = fc_layer(X)\n",
    "\n",
    "print(\"\\nSalida de la capa A:\")\n",
    "print(A)\n",
    "print(f\"A shape: {A.size()}\")\n",
    "\n",
    "# Verificar dimensiones\n",
    "print(f\"\\nResumen de dimensiones:\")\n",
    "print(f\"X: {X.size()} (10 ejemplos, 5 características)\")\n",
    "print(f\"W: {fc_layer.weight.size()} (3 neuronas, 5 entradas)\")\n",
    "print(f\"b: {fc_layer.bias.size()} (3 neuronas)\")\n",
    "print(f\"A: {A.size()} (10 ejemplos, 3 salidas)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Transformaciones Matriciales: Intuición Geométrica\n",
    "\n",
    "### 8.1 ¿Por qué la notación Wx es intuitiva?\n",
    "\n",
    "<!-- INSERT FIGURE: Page 37, 38, 39 - Matrix transformation visualizations -->\n",
    "\n",
    "Una matriz puede verse como una transformación que:\n",
    "- Escala coordenadas\n",
    "- Rota vectores\n",
    "- Cambia el espacio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo: Matriz identidad (no transforma)\n",
    "I = torch.tensor([[1., 0.],\n",
    "                  [0., 1.]])\n",
    "\n",
    "x = torch.tensor([[0.25],\n",
    "                  [0.5]])\n",
    "\n",
    "result = I @ x\n",
    "print(\"Matriz identidad × vector:\")\n",
    "print(result)\n",
    "print(\"(El vector no cambia)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo: Escalamiento\n",
    "# Escalar x por 3, y por 2\n",
    "S = torch.tensor([[3., 0.],\n",
    "                  [0., 2.]])\n",
    "\n",
    "x = torch.tensor([[1.],\n",
    "                  [1.]])\n",
    "\n",
    "result = S @ x\n",
    "print(\"Matriz de escalamiento × vector:\")\n",
    "print(f\"Original: {x.T}\")\n",
    "print(f\"Escalado: {result.T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Interpretación de Transformaciones\n",
    "\n",
    "Para una matriz general:\n",
    "\n",
    "$$\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = x\\begin{bmatrix} a \\\\ c \\end{bmatrix} + y\\begin{bmatrix} b \\\\ d \\end{bmatrix}$$\n",
    "\n",
    "Donde:\n",
    "- Primera columna: efecto en la dirección x\n",
    "- Segunda columna: efecto en la dirección y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz general\n",
    "W = torch.tensor([[2., 1.],\n",
    "                  [1., 3.]])\n",
    "\n",
    "x = torch.tensor([[1.],\n",
    "                  [1.]])\n",
    "\n",
    "result = W @ x\n",
    "\n",
    "print(\"Transformación:\")\n",
    "print(f\"W =\\n{W}\")\n",
    "print(f\"\\nx = {x.T}\")\n",
    "print(f\"\\nW @ x = {result.T}\")\n",
    "\n",
    "# Interpretación\n",
    "col1 = W[:, 0:1]\n",
    "col2 = W[:, 1:2]\n",
    "\n",
    "print(f\"\\nInterpretación:\")\n",
    "print(f\"x * col1 = {x[0, 0]} * {col1.T} = {(x[0, 0] * col1).T}\")\n",
    "print(f\"y * col2 = {x[1, 0]} * {col2.T} = {(x[1, 0] * col2).T}\")\n",
    "print(f\"Suma = {result.T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reglas Útiles para Conversión entre Teoría y Código\n",
    "\n",
    "Al cambiar entre la teoría matemática y el código de PyTorch, estas reglas son útiles:\n",
    "\n",
    "$$\\mathbf{AB} = (\\mathbf{B}^T\\mathbf{A}^T)^T$$\n",
    "\n",
    "$$(\\mathbf{AB})^T = \\mathbf{B}^T\\mathbf{A}^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar las reglas de transposición\n",
    "A = torch.randn(3, 4)\n",
    "B = torch.randn(4, 5)\n",
    "\n",
    "# Regla 1: AB = (B^T A^T)^T\n",
    "AB_direct = A @ B\n",
    "AB_rule1 = (B.T @ A.T).T\n",
    "\n",
    "print(\"Verificando regla 1: AB = (B^T A^T)^T\")\n",
    "print(f\"¿Son iguales? {torch.allclose(AB_direct, AB_rule1)}\")\n",
    "\n",
    "# Regla 2: (AB)^T = B^T A^T\n",
    "AB_T_direct = (A @ B).T\n",
    "AB_T_rule2 = B.T @ A.T\n",
    "\n",
    "print(\"\\nVerificando regla 2: (AB)^T = B^T A^T\")\n",
    "print(f\"¿Son iguales? {torch.allclose(AB_T_direct, AB_T_rule2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Resumen: Convenciones Tradicionales vs PyTorch\n",
    "\n",
    "<!-- INSERT FIGURE: Page 46 - Traditional vs PyTorch notation summary -->\n",
    "\n",
    "### Convención Tradicional (Matemática)\n",
    "\n",
    "**1 ejemplo:**\n",
    "$$\\sigma(\\mathbf{W}\\mathbf{x} + \\mathbf{b}) = \\mathbf{a}, \\quad \\mathbf{a} \\in \\mathbb{R}^{h \\times 1}, \\; \\mathbf{x} \\in \\mathbb{R}^{m \\times 1}$$\n",
    "\n",
    "**n ejemplos:**\n",
    "$$\\sigma([\\mathbf{W}\\mathbf{X}^T]^T + \\mathbf{b}) = \\mathbf{A}, \\quad \\mathbf{A} \\in \\mathbb{R}^{n \\times h}, \\; \\mathbf{X} \\in \\mathbb{R}^{n \\times m}$$\n",
    "\n",
    "### Convención PyTorch (Práctica)\n",
    "\n",
    "**1 ejemplo:**\n",
    "$$\\sigma([\\mathbf{x}^T\\mathbf{W}^T]^T + \\mathbf{b}) = \\mathbf{a}, \\quad \\mathbf{x} \\in \\mathbb{R}^{m \\times 1}$$\n",
    "\n",
    "Equivalente a:\n",
    "$$\\sigma([\\mathbf{x}\\mathbf{W}^T] + \\mathbf{b}) = \\mathbf{a}, \\quad \\mathbf{x} \\in \\mathbb{R}^{1 \\times m} \\text{ (PyTorch)}$$\n",
    "\n",
    "**n ejemplos:**\n",
    "$$\\sigma([\\mathbf{X}\\mathbf{W}^T] + \\mathbf{b}) = \\mathbf{A}, \\quad \\mathbf{X} \\in \\mathbb{R}^{n \\times m}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostración de la diferencia\n",
    "\n",
    "# Configuración\n",
    "n_samples = 5\n",
    "n_features = 3\n",
    "n_outputs = 2\n",
    "\n",
    "# Datos\n",
    "X = torch.randn(n_samples, n_features)  # PyTorch: (n × m)\n",
    "\n",
    "# Forma tradicional: W es (h × m)\n",
    "W_trad = torch.randn(n_outputs, n_features)\n",
    "\n",
    "# Forma PyTorch: W^T es (m × h), pero PyTorch lo almacena como (h × m)\n",
    "fc = torch.nn.Linear(n_features, n_outputs)\n",
    "\n",
    "print(f\"X shape (PyTorch): {X.shape} - ejemplos en filas\")\n",
    "print(f\"W shape (PyTorch Linear): {fc.weight.shape} - se usará transpuesta\")\n",
    "\n",
    "# Aplicar\n",
    "output_pytorch = fc(X)  # Internamente hace: X @ W^T + b\n",
    "print(f\"\\nOutput shape: {output_pytorch.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Ejercicio Práctico\n",
    "\n",
    "<!-- INSERT FIGURE: Page 47 - Exercise description -->\n",
    "\n",
    "**Ejercicio:** Revisa el código del perceptrón en NumPy:\n",
    "https://github.com/rasbt/stat453-deep-learning-ss20/blob/master/L03-perceptron/code/perceptron-numpy.ipynb\n",
    "\n",
    "**Preguntas:**\n",
    "\n",
    "1. Sin ejecutar el código, ¿puede el perceptrón predecir etiquetas si alimentamos un arreglo de múltiples ejemplos?\n",
    "   - ¿Sí? ¿Por qué?\n",
    "   - ¿No? ¿Qué cambio necesitarías?\n",
    "\n",
    "2. Ejecuta el código para verificar tu intuición.\n",
    "\n",
    "3. ¿Podemos tener paralelismo en el método `train` sin afectar la regla de aprendizaje?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementación simple del perceptrón para el ejercicio\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, num_features):\n",
    "        self.weights = torch.zeros(num_features)\n",
    "        self.bias = 0.\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Propagación hacia adelante\"\"\"\n",
    "        linear = torch.matmul(x, self.weights) + self.bias\n",
    "        predictions = torch.where(linear > 0., 1, 0)\n",
    "        return predictions\n",
    "    \n",
    "    def train_step(self, x, y):\n",
    "        \"\"\"Un paso de entrenamiento\"\"\"\n",
    "        predictions = self.forward(x)\n",
    "        errors = y - predictions\n",
    "        \n",
    "        # Actualización de pesos\n",
    "        self.weights += torch.matmul(x.T, errors.float())\n",
    "        self.bias += errors.sum().float()\n",
    "        \n",
    "        return errors.abs().sum().item()\n",
    "\n",
    "# Prueba con datos simples\n",
    "perceptron = Perceptron(num_features=2)\n",
    "\n",
    "# Un solo ejemplo\n",
    "x_single = torch.tensor([1., 2.])\n",
    "pred_single = perceptron.forward(x_single)\n",
    "print(f\"Predicción para un ejemplo: {pred_single}\")\n",
    "\n",
    "# Múltiples ejemplos\n",
    "X_batch = torch.tensor([[1., 2.],\n",
    "                        [2., 3.],\n",
    "                        [-1., -2.]])\n",
    "pred_batch = perceptron.forward(X_batch)\n",
    "print(f\"Predicciones para múltiples ejemplos: {pred_batch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Recursos Adicionales\n",
    "\n",
    "**Documentación oficial de PyTorch:**\n",
    "- Tutoriales: https://docs.pytorch.org/tutorials/\n",
    "- Documentación: https://pytorch.org/docs/\n",
    "\n",
    "**Para instalar PyTorch:**\n",
    "- Página oficial: https://pytorch.org\n",
    "- Herramienta de selección de versión disponible\n",
    "- Para CPU (portátiles): selecciona \"None\" en CUDA\n",
    "- Comando recomendado: `conda install pytorch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "### Puntos Clave:\n",
    "\n",
    "1. **Piensa siempre en productos punto** al escribir e implementar multiplicación de matrices\n",
    "\n",
    "2. **La intuición teórica y la convención no siempre coinciden** con la conveniencia práctica al programar\n",
    "\n",
    "3. **Reglas útiles para conversión:**\n",
    "   - $\\mathbf{AB} = (\\mathbf{B}^T\\mathbf{A}^T)^T$\n",
    "   - $(\\mathbf{AB})^T = \\mathbf{B}^T\\mathbf{A}^T$\n",
    "\n",
    "4. **PyTorch vs NumPy:**\n",
    "   - PyTorch ofrece soporte GPU\n",
    "   - Diferenciación automática\n",
    "   - Herramientas especializadas para deep learning\n",
    "\n",
    "5. **Broadcasting** hace las operaciones más convenientes pero hay que entender cómo funciona\n",
    "\n",
    "6. **Convención PyTorch:** Los ejemplos van en filas, PyTorch usa $\\mathbf{X}\\mathbf{W}^T$ internamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ¡Gracias!\n",
    "\n",
    "<!-- INSERT FIGURE: Page 49 - Thank you slide -->\n",
    "\n",
    "**Universidad EAFIT**\n",
    "\n",
    "Escuela de Ciencias Aplicadas e Ingeniería"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
