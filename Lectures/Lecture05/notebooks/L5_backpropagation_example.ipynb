{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oat7JsHQSoqP",
        "outputId": "7f3e9b40-22fa-4b3f-9431-a8a4e672d1b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Forward ===\n",
            "yhat1=0.751365069552, yhat2=0.772928465321\n",
            "Total loss E=0.298371108760\n",
            "\n",
            "=== Backward (key deltas) ===\n",
            "delta_o1=0.138498561629, delta_o2=-0.038098236517\n",
            "delta_h1=0.008771354689, delta_h2=0.009954254705\n",
            "\n",
            "=== Updated params ===\n",
            "w1_11: 0.150000000000 -> 0.149780716133\n",
            "w1_12: 0.250000000000 -> 0.249751143632\n",
            "w1_21: 0.200000000000 -> 0.199561432266\n",
            "w1_22: 0.300000000000 -> 0.299502287265\n",
            "b1: 0.350000000000 -> 0.340637195303\n",
            "w2_11: 0.400000000000 -> 0.358916479718\n",
            "w2_12: 0.500000000000 -> 0.511301270239\n",
            "w2_21: 0.450000000000 -> 0.408666186076\n",
            "w2_22: 0.550000000000 -> 0.561370121108\n",
            "b2: 0.600000000000 -> 0.549799837444\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Backpropagation demo (2-2-2 network) using:\n",
        "- Inputs: x1=0.05, x2=0.10\n",
        "- Targets: y1=0.01, y2=0.99\n",
        "- Sigmoid activations (hidden + output)\n",
        "- Loss: sum of 1/2 (y - yhat)^2 (MSE per output, summed)\n",
        "- Learning rate alpha=0.5\n",
        "\n",
        "This matches your slide setup (same initial weights/biases).\n",
        "\"\"\"\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from math import exp\n",
        "\n",
        "def sigmoid(z: float) -> float:\n",
        "    return 1.0 / (1.0 + exp(-z))\n",
        "\n",
        "def sigmoid_prime_from_a(a: float) -> float:\n",
        "    # derivative using activation: σ'(z) = a(1-a)\n",
        "    return a * (1.0 - a)\n",
        "\n",
        "@dataclass\n",
        "class Params:\n",
        "    # input -> hidden\n",
        "    w1_11: float  # x1 -> h1\n",
        "    w1_21: float  # x2 -> h1\n",
        "    w1_12: float  # x1 -> h2\n",
        "    w1_22: float  # x2 -> h2\n",
        "    b1: float     # hidden bias (shared)\n",
        "\n",
        "    # hidden -> output\n",
        "    w2_11: float  # h1 -> o1\n",
        "    w2_21: float  # h2 -> o1\n",
        "    w2_12: float  # h1 -> o2\n",
        "    w2_22: float  # h2 -> o2\n",
        "    b2: float     # output bias (shared)\n",
        "\n",
        "def forward(x1: float, x2: float, p: Params):\n",
        "    # Hidden pre-activations\n",
        "    z_h1 = x1 * p.w1_11 + x2 * p.w1_21 + p.b1\n",
        "    z_h2 = x1 * p.w1_12 + x2 * p.w1_22 + p.b1\n",
        "    # Hidden activations\n",
        "    a_h1 = sigmoid(z_h1)\n",
        "    a_h2 = sigmoid(z_h2)\n",
        "\n",
        "    # Output pre-activations\n",
        "    z_o1 = a_h1 * p.w2_11 + a_h2 * p.w2_21 + p.b2\n",
        "    z_o2 = a_h1 * p.w2_12 + a_h2 * p.w2_22 + p.b2\n",
        "    # Output activations (predictions)\n",
        "    yhat1 = sigmoid(z_o1)\n",
        "    yhat2 = sigmoid(z_o2)\n",
        "\n",
        "    cache = {\n",
        "        \"z_h1\": z_h1, \"z_h2\": z_h2, \"a_h1\": a_h1, \"a_h2\": a_h2,\n",
        "        \"z_o1\": z_o1, \"z_o2\": z_o2, \"yhat1\": yhat1, \"yhat2\": yhat2,\n",
        "        \"x1\": x1, \"x2\": x2\n",
        "    }\n",
        "    return yhat1, yhat2, cache\n",
        "\n",
        "def loss_mse_sum(y1: float, y2: float, yhat1: float, yhat2: float) -> float:\n",
        "    return 0.5 * (y1 - yhat1) ** 2 + 0.5 * (y2 - yhat2) ** 2\n",
        "\n",
        "def backward(y1: float, y2: float, p: Params, cache):\n",
        "    \"\"\"\n",
        "    Compute gradients for all params using chain rule.\n",
        "\n",
        "    For output layer with MSE + sigmoid:\n",
        "      dL/dyhat = (yhat - y)\n",
        "      dyhat/dz = yhat(1-yhat)\n",
        "      => delta_out = dL/dz = (yhat - y) * yhat(1-yhat)\n",
        "\n",
        "    Then:\n",
        "      dL/dw2_ij = a_hj * delta_out_i\n",
        "      dL/db2    = delta_out_1 + delta_out_2  (since b2 is shared)\n",
        "    Hidden deltas:\n",
        "      delta_h = (W2 * delta_out) ⊙ a_h(1-a_h)\n",
        "      where contribution to h1 is w2_11*delta1 + w2_12*delta2\n",
        "            contribution to h2 is w2_21*delta1 + w2_22*delta2\n",
        "    Then:\n",
        "      dL/dw1 = x * delta_h\n",
        "      dL/db1 = delta_h1 + delta_h2 (since b1 is shared)\n",
        "    \"\"\"\n",
        "    x1, x2 = cache[\"x1\"], cache[\"x2\"]\n",
        "    a_h1, a_h2 = cache[\"a_h1\"], cache[\"a_h2\"]\n",
        "    yhat1, yhat2 = cache[\"yhat1\"], cache[\"yhat2\"]\n",
        "\n",
        "    # Output deltas\n",
        "    dL_dyhat1 = (yhat1 - y1)\n",
        "    dL_dyhat2 = (yhat2 - y2)\n",
        "    dyhat1_dz = sigmoid_prime_from_a(yhat1)\n",
        "    dyhat2_dz = sigmoid_prime_from_a(yhat2)\n",
        "    delta_o1 = dL_dyhat1 * dyhat1_dz\n",
        "    delta_o2 = dL_dyhat2 * dyhat2_dz\n",
        "\n",
        "    # Gradients: hidden -> output\n",
        "    dL_dw2_11 = a_h1 * delta_o1\n",
        "    dL_dw2_21 = a_h2 * delta_o1\n",
        "    dL_dw2_12 = a_h1 * delta_o2\n",
        "    dL_dw2_22 = a_h2 * delta_o2\n",
        "    dL_db2    = delta_o1 + delta_o2  # shared bias\n",
        "\n",
        "    # Hidden deltas\n",
        "    da_h1 = (p.w2_11 * delta_o1) + (p.w2_12 * delta_o2)\n",
        "    da_h2 = (p.w2_21 * delta_o1) + (p.w2_22 * delta_o2)\n",
        "    delta_h1 = da_h1 * sigmoid_prime_from_a(a_h1)\n",
        "    delta_h2 = da_h2 * sigmoid_prime_from_a(a_h2)\n",
        "\n",
        "    # Gradients: input -> hidden\n",
        "    dL_dw1_11 = x1 * delta_h1\n",
        "    dL_dw1_21 = x2 * delta_h1\n",
        "    dL_dw1_12 = x1 * delta_h2\n",
        "    dL_dw1_22 = x2 * delta_h2\n",
        "    dL_db1    = delta_h1 + delta_h2  # shared bias\n",
        "\n",
        "    grads = {\n",
        "        \"w2_11\": dL_dw2_11, \"w2_21\": dL_dw2_21, \"w2_12\": dL_dw2_12, \"w2_22\": dL_dw2_22,\n",
        "        \"b2\": dL_db2,\n",
        "        \"w1_11\": dL_dw1_11, \"w1_21\": dL_dw1_21, \"w1_12\": dL_dw1_12, \"w1_22\": dL_dw1_22,\n",
        "        \"b1\": dL_db1,\n",
        "        \"delta_o1\": delta_o1, \"delta_o2\": delta_o2,\n",
        "        \"delta_h1\": delta_h1, \"delta_h2\": delta_h2,\n",
        "    }\n",
        "    return grads\n",
        "\n",
        "def update_params(p: Params, grads: dict, alpha: float) -> Params:\n",
        "    # Gradient descent step\n",
        "    return Params(\n",
        "        w1_11 = p.w1_11 - alpha * grads[\"w1_11\"],\n",
        "        w1_21 = p.w1_21 - alpha * grads[\"w1_21\"],\n",
        "        w1_12 = p.w1_12 - alpha * grads[\"w1_12\"],\n",
        "        w1_22 = p.w1_22 - alpha * grads[\"w1_22\"],\n",
        "        b1    = p.b1    - alpha * grads[\"b1\"],\n",
        "        w2_11 = p.w2_11 - alpha * grads[\"w2_11\"],\n",
        "        w2_21 = p.w2_21 - alpha * grads[\"w2_21\"],\n",
        "        w2_12 = p.w2_12 - alpha * grads[\"w2_12\"],\n",
        "        w2_22 = p.w2_22 - alpha * grads[\"w2_22\"],\n",
        "        b2    = p.b2    - alpha * grads[\"b2\"],\n",
        "    )\n",
        "\n",
        "def run_one_step(x1, x2, y1, y2, alpha, p: Params, verbose=True):\n",
        "    yhat1, yhat2, cache = forward(x1, x2, p)\n",
        "    E = loss_mse_sum(y1, y2, yhat1, yhat2)\n",
        "    grads = backward(y1, y2, p, cache)\n",
        "    p_new = update_params(p, grads, alpha)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"=== Forward ===\")\n",
        "        print(f\"yhat1={yhat1:.12f}, yhat2={yhat2:.12f}\")\n",
        "        print(f\"Total loss E={E:.12f}\\n\")\n",
        "\n",
        "        print(\"=== Backward (key deltas) ===\")\n",
        "        print(f\"delta_o1={grads['delta_o1']:.12f}, delta_o2={grads['delta_o2']:.12f}\")\n",
        "        print(f\"delta_h1={grads['delta_h1']:.12f}, delta_h2={grads['delta_h2']:.12f}\\n\")\n",
        "\n",
        "        print(\"=== Updated params ===\")\n",
        "        for k in [\"w1_11\",\"w1_12\",\"w1_21\",\"w1_22\",\"b1\",\"w2_11\",\"w2_12\",\"w2_21\",\"w2_22\",\"b2\"]:\n",
        "            old = getattr(p, k)\n",
        "            new = getattr(p_new, k)\n",
        "            print(f\"{k}: {old:.12f} -> {new:.12f}\")\n",
        "\n",
        "    return p_new, E, (yhat1, yhat2), grads\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Your data\n",
        "    x1, x2 = 0.05, 0.10\n",
        "    y1, y2 = 0.01, 0.99\n",
        "    alpha = 0.5\n",
        "\n",
        "    # Your initialization (as in the slides)\n",
        "    p = Params(\n",
        "        w1_11=0.15, w1_12=0.25,\n",
        "        w1_21=0.20, w1_22=0.30,\n",
        "        b1=0.35,\n",
        "        w2_11=0.40, w2_12=0.50,\n",
        "        w2_21=0.45, w2_22=0.55,\n",
        "        b2=0.60\n",
        "    )\n",
        "\n",
        "    run_one_step(x1, x2, y1, y2, alpha, p, verbose=True)\n"
      ]
    }
  ]
}