{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression: Normal Equations and Pseudo-Inverse\n",
    "\n",
    "This notebook provides a comprehensive explanation of linear regression using the normal equations approach, covering both univariate and multivariate cases, with theoretical derivations and practical implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "np.random.seed(42)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Linear Regression\n",
    "\n",
    "Linear regression models the relationship between input features and a target variable using a linear function:\n",
    "\n",
    "$$y = w_0 + w_1 x_1 + w_2 x_2 + \\cdots + w_n x_n$$\n",
    "\n",
    "Where:\n",
    "- $y$ is the predicted output\n",
    "- $x_1, x_2, \\ldots, x_n$ are input features\n",
    "- $w_0$ is the bias (intercept)\n",
    "- $w_1, w_2, \\ldots, w_n$ are the weights (coefficients)\n",
    "\n",
    "### The Normal Equation\n",
    "\n",
    "The closed-form solution for linear regression is given by:\n",
    "\n",
    "$$\\mathbf{w} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}$$\n",
    "\n",
    "This assumes the bias is included in $\\mathbf{w}$ and the design matrix $\\mathbf{X}$ has an additional column of ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Univariate Linear Regression\n",
    "\n",
    "### 2.1 Theory\n",
    "\n",
    "For a single feature, the model is:\n",
    "\n",
    "$$y = w_0 + w_1 x$$\n",
    "\n",
    "In matrix form with $m$ training examples:\n",
    "\n",
    "$$\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_m \\end{bmatrix} = \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_m \\end{bmatrix} \\begin{bmatrix} w_0 \\\\ w_1 \\end{bmatrix}$$\n",
    "\n",
    "Or simply: $\\mathbf{y} = \\mathbf{X} \\mathbf{w}$\n",
    "\n",
    "### 2.2 Creating the Design Matrix\n",
    "\n",
    "The design matrix $\\mathbf{X}$ includes:\n",
    "1. A column of ones (for the bias term $w_0$)\n",
    "2. The feature column(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate synthetic univariate data\n",
    "m = 100  # number of samples\n",
    "x = np.linspace(0, 10, m)\n",
    "y_true = 2.5 * x + 1.0\n",
    "y = y_true + np.random.randn(m) * 2  # Add noise\n",
    "\n",
    "# Create design matrix by adding a column of ones\n",
    "X = np.c_[np.ones(m), x]  # Shape: (m, 2)\n",
    "\n",
    "print(f\"Data shape: x={x.shape}, y={y.shape}\")\n",
    "print(f\"Design matrix X shape: {X.shape}\")\n",
    "print(f\"\\nFirst 5 rows of X:\\n{X[:5]}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x, y, alpha=0.6, label='Data points')\n",
    "plt.plot(x, y_true, 'r--', linewidth=2, label='True relationship')\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Univariate Linear Regression Data', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Solving Using Normal Equations\n",
    "\n",
    "The normal equation is derived by minimizing the squared error:\n",
    "\n",
    "$$L(\\mathbf{w}) = \\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|^2 = (\\mathbf{y} - \\mathbf{X}\\mathbf{w})^\\top(\\mathbf{y} - \\mathbf{X}\\mathbf{w})$$\n",
    "\n",
    "Taking the gradient and setting it to zero:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\mathbf{w}} = -2\\mathbf{X}^\\top(\\mathbf{y} - \\mathbf{X}\\mathbf{w}) = 0$$\n",
    "\n",
    "$$\\mathbf{X}^\\top \\mathbf{X} \\mathbf{w} = \\mathbf{X}^\\top \\mathbf{y}$$\n",
    "\n",
    "$$\\mathbf{w} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Solve using normal equations\n",
    "def normal_equation(X, y):\n",
    "    \"\"\"\n",
    "    Solve linear regression using normal equations.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : ndarray, shape (m, n+1)\n",
    "        Design matrix with bias column\n",
    "    y : ndarray, shape (m,)\n",
    "        Target values\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    w : ndarray, shape (n+1,)\n",
    "        Optimal weights including bias\n",
    "    \"\"\"\n",
    "    return np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "# Calculate weights\n",
    "w = normal_equation(X, y)\n",
    "\n",
    "print(f\"Learned weights: w = {w}\")\n",
    "print(f\"w₀ (bias) = {w[0]:.4f}\")\n",
    "print(f\"w₁ (slope) = {w[1]:.4f}\")\n",
    "print(f\"\\nTrue values: w₀ = 1.0, w₁ = 2.5\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize the fit\n",
    "y_pred = X @ w\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x, y, alpha=0.6, label='Data points')\n",
    "plt.plot(x, y_true, 'r--', linewidth=2, label='True relationship')\n",
    "plt.plot(x, y_pred, 'g-', linewidth=2, label='Fitted line')\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Univariate Linear Regression: Fitted Model', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Calculate error metrics\n",
    "mse = np.mean((y - y_pred)**2)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"\\nMean Squared Error: {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multivariate Linear Regression\n",
    "\n",
    "### 3.1 Theory\n",
    "\n",
    "For multiple features, the model extends to:\n",
    "\n",
    "$$y = w_0 + w_1 x_1 + w_2 x_2 + \\cdots + w_n x_n$$\n",
    "\n",
    "In matrix form:\n",
    "\n",
    "$$\\mathbf{y} = \\mathbf{X} \\mathbf{w}$$\n",
    "\n",
    "Where $\\mathbf{X}$ is now $(m \\times (n+1))$ matrix:\n",
    "\n",
    "$$\\mathbf{X} = \\begin{bmatrix} \n",
    "1 & x_{1,1} & x_{1,2} & \\cdots & x_{1,n} \\\\\n",
    "1 & x_{2,1} & x_{2,2} & \\cdots & x_{2,n} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & x_{m,1} & x_{m,2} & \\cdots & x_{m,n}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The normal equation solution remains the same:\n",
    "\n",
    "$$\\mathbf{w} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate synthetic multivariate data (2 features)\n",
    "m = 200  # number of samples\n",
    "n = 2    # number of features\n",
    "\n",
    "X_raw = np.random.randn(m, n)\n",
    "X_raw[:, 0] = X_raw[:, 0] * 2 + 1  # Scale feature 1\n",
    "X_raw[:, 1] = X_raw[:, 1] * 3 - 2  # Scale feature 2\n",
    "\n",
    "# True weights\n",
    "w_true = np.array([5.0, 2.0, -3.0])  # [bias, w1, w2]\n",
    "\n",
    "# Create design matrix\n",
    "X_multi = np.c_[np.ones(m), X_raw]  # Add bias column\n",
    "\n",
    "# Generate target with noise\n",
    "y_multi = X_multi @ w_true + np.random.randn(m) * 2\n",
    "\n",
    "print(f\"Design matrix X shape: {X_multi.shape}\")\n",
    "print(f\"Target y shape: {y_multi.shape}\")\n",
    "print(f\"\\nFirst 5 rows of X:\\n{X_multi[:5]}\")\n",
    "print(f\"\\nTrue weights: {w_true}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize the data in 3D\n",
    "fig = plt.figure(figsize=(12, 9))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(X_raw[:, 0], X_raw[:, 1], y_multi, c=y_multi, \n",
    "           cmap='viridis', alpha=0.6, s=20)\n",
    "ax.set_xlabel('Feature 1 (x₁)', fontsize=11)\n",
    "ax.set_ylabel('Feature 2 (x₂)', fontsize=11)\n",
    "ax.set_zlabel('Target (y)', fontsize=11)\n",
    "ax.set_title('Multivariate Linear Regression Data', fontsize=14)\n",
    "plt.colorbar(ax.scatter(X_raw[:, 0], X_raw[:, 1], y_multi, \n",
    "                        c=y_multi, cmap='viridis', alpha=0.6), \n",
    "             ax=ax, label='y value')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Solving Multivariate Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Solve using normal equations\n",
    "w_learned = normal_equation(X_multi, y_multi)\n",
    "\n",
    "print(\"Learned weights:\")\n",
    "print(f\"  w₀ (bias)  = {w_learned[0]:.4f}  (true: {w_true[0]:.4f})\")\n",
    "print(f\"  w₁ (feat1) = {w_learned[1]:.4f}  (true: {w_true[1]:.4f})\")\n",
    "print(f\"  w₂ (feat2) = {w_learned[2]:.4f}  (true: {w_true[2]:.4f})\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_multi = X_multi @ w_learned\n",
    "\n",
    "# Calculate metrics\n",
    "mse_multi = np.mean((y_multi - y_pred_multi)**2)\n",
    "rmse_multi = np.sqrt(mse_multi)\n",
    "r2 = 1 - np.sum((y_multi - y_pred_multi)**2) / np.sum((y_multi - y_multi.mean())**2)\n",
    "\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"  MSE:  {mse_multi:.4f}\")\n",
    "print(f\"  RMSE: {rmse_multi:.4f}\")\n",
    "print(f\"  R²:   {r2:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize the fitted plane\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot data points\n",
    "ax.scatter(X_raw[:, 0], X_raw[:, 1], y_multi, \n",
    "           c='blue', alpha=0.5, s=20, label='Data points')\n",
    "\n",
    "# Create mesh for the fitted plane\n",
    "x1_range = np.linspace(X_raw[:, 0].min(), X_raw[:, 0].max(), 20)\n",
    "x2_range = np.linspace(X_raw[:, 1].min(), X_raw[:, 1].max(), 20)\n",
    "X1_mesh, X2_mesh = np.meshgrid(x1_range, x2_range)\n",
    "Y_mesh = w_learned[0] + w_learned[1] * X1_mesh + w_learned[2] * X2_mesh\n",
    "\n",
    "# Plot the fitted plane\n",
    "ax.plot_surface(X1_mesh, X2_mesh, Y_mesh, alpha=0.3, \n",
    "                cmap='viridis', label='Fitted plane')\n",
    "\n",
    "ax.set_xlabel('Feature 1 (x₁)', fontsize=11)\n",
    "ax.set_ylabel('Feature 2 (x₂)', fontsize=11)\n",
    "ax.set_zlabel('Target (y)', fontsize=11)\n",
    "ax.set_title('Multivariate Linear Regression: Fitted Model', fontsize=14)\n",
    "ax.legend()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Predicted vs Actual plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_multi, y_pred_multi, alpha=0.5)\n",
    "plt.plot([y_multi.min(), y_multi.max()], \n",
    "         [y_multi.min(), y_multi.max()], \n",
    "         'r--', linewidth=2, label='Perfect prediction')\n",
    "plt.xlabel('Actual y', fontsize=12)\n",
    "plt.ylabel('Predicted y', fontsize=12)\n",
    "plt.title('Predicted vs Actual Values', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Pseudo-Inverse (Moore-Penrose Inverse)\n",
    "\n",
    "### 4.1 Theory and Derivation\n",
    "\n",
    "The term $(\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top$ in the normal equation is called the **pseudo-inverse** or **Moore-Penrose inverse** of $\\mathbf{X}$, denoted as $\\mathbf{X}^+$.\n",
    "\n",
    "$$\\mathbf{X}^+ = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top$$\n",
    "\n",
    "So the solution becomes:\n",
    "\n",
    "$$\\mathbf{w} = \\mathbf{X}^+ \\mathbf{y}$$\n",
    "\n",
    "#### Properties of the Pseudo-Inverse:\n",
    "\n",
    "1. **For full rank matrices**: If $\\mathbf{X}$ has full column rank (i.e., $\\mathbf{X}^\\top \\mathbf{X}$ is invertible), then:\n",
    "   $$\\mathbf{X}^+ = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top$$\n",
    "\n",
    "2. **For any matrix**: The pseudo-inverse always exists, even when $\\mathbf{X}$ is singular or not square.\n",
    "\n",
    "3. **Minimum norm solution**: $\\mathbf{X}^+$ gives the least squares solution with minimum norm.\n",
    "\n",
    "4. **Key properties**:\n",
    "   - $\\mathbf{X} \\mathbf{X}^+ \\mathbf{X} = \\mathbf{X}$\n",
    "   - $\\mathbf{X}^+ \\mathbf{X} \\mathbf{X}^+ = \\mathbf{X}^+$\n",
    "   - $(\\mathbf{X} \\mathbf{X}^+)^\\top = \\mathbf{X} \\mathbf{X}^+$\n",
    "   - $(\\mathbf{X}^+ \\mathbf{X})^\\top = \\mathbf{X}^+ \\mathbf{X}$\n",
    "\n",
    "#### SVD-based Computation:\n",
    "\n",
    "The pseudo-inverse can be computed using Singular Value Decomposition (SVD):\n",
    "\n",
    "If $\\mathbf{X} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^\\top$ (SVD decomposition), then:\n",
    "\n",
    "$$\\mathbf{X}^+ = \\mathbf{V} \\mathbf{\\Sigma}^+ \\mathbf{U}^\\top$$\n",
    "\n",
    "where $\\mathbf{\\Sigma}^+$ is formed by taking the reciprocal of all non-zero diagonal elements of $\\mathbf{\\Sigma}$ and transposing.\n",
    "\n",
    "### 4.2 Why Use the Pseudo-Inverse?\n",
    "\n",
    "1. **Numerical stability**: More robust when $\\mathbf{X}^\\top \\mathbf{X}$ is ill-conditioned\n",
    "2. **Handles rank-deficiency**: Works even when columns of $\\mathbf{X}$ are linearly dependent\n",
    "3. **Handles overdetermined and underdetermined systems**\n",
    "4. **Automatic regularization**: SVD-based computation can threshold small singular values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Implementation Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def solve_normal_equation_manual(X, y):\n",
    "    \"\"\"Solve using explicit matrix inversion.\"\"\"\n",
    "    return np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "def solve_pinv(X, y):\n",
    "    \"\"\"Solve using pseudo-inverse.\"\"\"\n",
    "    return np.linalg.pinv(X) @ y\n",
    "\n",
    "def solve_svd(X, y):\n",
    "    \"\"\"Solve using SVD decomposition explicitly.\"\"\"\n",
    "    U, s, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "    # Compute pseudo-inverse via SVD\n",
    "    S_inv = np.diag(1 / s)\n",
    "    X_pinv = Vt.T @ S_inv @ U.T\n",
    "    return X_pinv @ y\n",
    "\n",
    "def solve_lstsq(X, y):\n",
    "    \"\"\"Solve using numpy's least squares (uses SVD internally).\"\"\"\n",
    "    return np.linalg.lstsq(X, y, rcond=None)[0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare all methods on the multivariate dataset\n",
    "import time\n",
    "\n",
    "methods = {\n",
    "    'Normal Equation (manual)': solve_normal_equation_manual,\n",
    "    'Pseudo-inverse (pinv)': solve_pinv,\n",
    "    'SVD (manual)': solve_svd,\n",
    "    'Least Squares (lstsq)': solve_lstsq\n",
    "}\n",
    "\n",
    "print(\"Comparing different solution methods:\\n\")\n",
    "print(f\"{'Method':<30} {'w0 (bias)':<12} {'w1':<12} {'w2':<12} {'Time (ms)':<12}\")\n",
    "print(\"-\" * 78)\n",
    "\n",
    "for name, method in methods.items():\n",
    "    start = time.time()\n",
    "    w_method = method(X_multi, y_multi)\n",
    "    elapsed = (time.time() - start) * 1000  # Convert to milliseconds\n",
    "    \n",
    "    print(f\"{name:<30} {w_method[0]:<12.4f} {w_method[1]:<12.4f} {w_method[2]:<12.4f} {elapsed:<12.4f}\")\n",
    "\n",
    "print(f\"\\n{'True weights:':<30} {w_true[0]:<12.4f} {w_true[1]:<12.4f} {w_true[2]:<12.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Understanding the SVD Approach\n",
    "\n",
    "Let's visualize the SVD decomposition and how it computes the pseudo-inverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Perform SVD on design matrix\n",
    "U, s, Vt = np.linalg.svd(X_multi, full_matrices=False)\n",
    "\n",
    "print(\"SVD Decomposition of X:\")\n",
    "print(f\"\\nX shape: {X_multi.shape}\")\n",
    "print(f\"U shape: {U.shape}  (left singular vectors)\")\n",
    "print(f\"s shape: {s.shape}  (singular values)\")\n",
    "print(f\"Vt shape: {Vt.shape}  (right singular vectors transposed)\")\n",
    "print(f\"\\nSingular values: {s}\")\n",
    "print(f\"Condition number: {s[0] / s[-1]:.2f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize singular values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(s)), s)\n",
    "plt.xlabel('Singular Value Index', fontsize=12)\n",
    "plt.ylabel('Magnitude', fontsize=12)\n",
    "plt.title('Singular Values of Design Matrix X', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Show how pseudo-inverse is computed\n",
    "print(\"\\nPseudo-inverse computation:\")\n",
    "print(f\"Original singular values: {s}\")\n",
    "print(f\"Reciprocals (Σ⁺): {1/s}\")\n",
    "\n",
    "# Reconstruct X to verify SVD\n",
    "X_reconstructed = U @ np.diag(s) @ Vt\n",
    "reconstruction_error = np.linalg.norm(X_multi - X_reconstructed)\n",
    "print(f\"\\nReconstruction error: {reconstruction_error:.10f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Handling Ill-Conditioned Matrices\n",
    "\n",
    "When $\\mathbf{X}^\\top \\mathbf{X}$ is nearly singular (ill-conditioned), the pseudo-inverse is more stable than direct matrix inversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create an ill-conditioned design matrix\n",
    "m_ill = 50\n",
    "X_ill_raw = np.random.randn(m_ill, 2)\n",
    "# Make columns nearly collinear\n",
    "X_ill_raw[:, 1] = X_ill_raw[:, 0] + np.random.randn(m_ill) * 0.01\n",
    "\n",
    "X_ill = np.c_[np.ones(m_ill), X_ill_raw]\n",
    "y_ill = X_ill @ np.array([3.0, 2.0, -1.0]) + np.random.randn(m_ill) * 0.5\n",
    "\n",
    "# Compute condition number\n",
    "cond_num = np.linalg.cond(X_ill.T @ X_ill)\n",
    "print(f\"Condition number of X^T X: {cond_num:.2e}\")\n",
    "print(f\"Matrix is {'ill-conditioned' if cond_num > 1e10 else 'well-conditioned'}\")\n",
    "\n",
    "# Correlation between features\n",
    "correlation = np.corrcoef(X_ill_raw.T)[0, 1]\n",
    "print(f\"\\nCorrelation between features: {correlation:.6f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare stability of methods\n",
    "print(\"\\nComparing methods on ill-conditioned matrix:\\n\")\n",
    "\n",
    "try:\n",
    "    w_normal = solve_normal_equation_manual(X_ill, y_ill)\n",
    "    print(f\"Normal equation:    {w_normal}\")\n",
    "except np.linalg.LinAlgError as e:\n",
    "    print(f\"Normal equation:    Failed - {e}\")\n",
    "\n",
    "w_pinv = solve_pinv(X_ill, y_ill)\n",
    "print(f\"Pseudo-inverse:     {w_pinv}\")\n",
    "\n",
    "w_svd = solve_svd(X_ill, y_ill)\n",
    "print(f\"SVD:                {w_svd}\")\n",
    "\n",
    "w_lstsq = solve_lstsq(X_ill, y_ill)\n",
    "print(f\"Least squares:      {w_lstsq}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Computational Complexity Analysis\n",
    "\n",
    "### Time Complexity:\n",
    "\n",
    "For a design matrix $\\mathbf{X}$ of size $m \\times n$:\n",
    "\n",
    "1. **Normal Equation**: $O(n^2 m + n^3)$\n",
    "   - Computing $\\mathbf{X}^\\top \\mathbf{X}$: $O(n^2 m)$\n",
    "   - Matrix inversion: $O(n^3)$\n",
    "   - Final multiplication: $O(n^2 m)$\n",
    "\n",
    "2. **SVD-based**: $O(mn^2)$ or $O(m^2 n)$\n",
    "   - Generally more stable but slower for $n \\ll m$\n",
    "\n",
    "3. **For large datasets**: Use iterative methods like gradient descent\n",
    "\n",
    "### When to use each method:\n",
    "\n",
    "- **Normal equation**: When $n$ is small (< 10,000) and $\\mathbf{X}^\\top \\mathbf{X}$ is well-conditioned\n",
    "- **Pseudo-inverse/SVD**: When numerical stability is critical or matrix is ill-conditioned\n",
    "- **Gradient descent**: When $n$ or $m$ is very large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Benchmark different problem sizes\n",
    "sizes = [10, 50, 100, 500, 1000]\n",
    "times_normal = []\n",
    "times_pinv = []\n",
    "times_lstsq = []\n",
    "\n",
    "for n_features in sizes:\n",
    "    m_samples = n_features * 5\n",
    "    X_bench = np.random.randn(m_samples, n_features + 1)\n",
    "    X_bench[:, 0] = 1  # Bias column\n",
    "    y_bench = np.random.randn(m_samples)\n",
    "    \n",
    "    # Normal equation\n",
    "    start = time.time()\n",
    "    _ = solve_normal_equation_manual(X_bench, y_bench)\n",
    "    times_normal.append(time.time() - start)\n",
    "    \n",
    "    # Pseudo-inverse\n",
    "    start = time.time()\n",
    "    _ = solve_pinv(X_bench, y_bench)\n",
    "    times_pinv.append(time.time() - start)\n",
    "    \n",
    "    # Least squares\n",
    "    start = time.time()\n",
    "    _ = solve_lstsq(X_bench, y_bench)\n",
    "    times_lstsq.append(time.time() - start)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.plot(sizes, times_normal, 'o-', linewidth=2, markersize=8, label='Normal Equation')\n",
    "plt.plot(sizes, times_pinv, 's-', linewidth=2, markersize=8, label='Pseudo-inverse')\n",
    "plt.plot(sizes, times_lstsq, '^-', linewidth=2, markersize=8, label='Least Squares')\n",
    "plt.xlabel('Number of Features', fontsize=12)\n",
    "plt.ylabel('Computation Time (seconds)', fontsize=12)\n",
    "plt.title('Computational Complexity: Time vs Problem Size', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Practical Implementation Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class LinearRegression:\n",
    "    \"\"\"\n",
    "    Linear Regression using Normal Equations.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    method : str, default='pinv'\n",
    "        Method for solving: 'normal', 'pinv', 'svd', or 'lstsq'\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, method='pinv'):\n",
    "        self.method = method\n",
    "        self.weights = None\n",
    "        self.n_features = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the linear regression model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (m, n)\n",
    "            Training features\n",
    "        y : array-like, shape (m,)\n",
    "            Target values\n",
    "        \"\"\"\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        \n",
    "        # Add bias column\n",
    "        X_with_bias = np.c_[np.ones(X.shape[0]), X]\n",
    "        self.n_features = X.shape[1]\n",
    "        \n",
    "        # Solve using specified method\n",
    "        if self.method == 'normal':\n",
    "            self.weights = np.linalg.inv(X_with_bias.T @ X_with_bias) @ X_with_bias.T @ y\n",
    "        elif self.method == 'pinv':\n",
    "            self.weights = np.linalg.pinv(X_with_bias) @ y\n",
    "        elif self.method == 'svd':\n",
    "            U, s, Vt = np.linalg.svd(X_with_bias, full_matrices=False)\n",
    "            S_inv = np.diag(1 / s)\n",
    "            self.weights = Vt.T @ S_inv @ U.T @ y\n",
    "        elif self.method == 'lstsq':\n",
    "            self.weights = np.linalg.lstsq(X_with_bias, y, rcond=None)[0]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {self.method}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (m, n)\n",
    "            Input features\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        y_pred : array, shape (m,)\n",
    "            Predicted values\n",
    "        \"\"\"\n",
    "        if self.weights is None:\n",
    "            raise ValueError(\"Model not fitted yet. Call fit() first.\")\n",
    "        \n",
    "        X = np.asarray(X)\n",
    "        X_with_bias = np.c_[np.ones(X.shape[0]), X]\n",
    "        return X_with_bias @ self.weights\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate R² score.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (m, n)\n",
    "            Test features\n",
    "        y : array-like, shape (m,)\n",
    "            True values\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        r2 : float\n",
    "            R² score\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        ss_res = np.sum((y - y_pred) ** 2)\n",
    "        ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "        return 1 - (ss_res / ss_tot)\n",
    "    \n",
    "    @property\n",
    "    def bias(self):\n",
    "        \"\"\"Return the bias term.\"\"\"\n",
    "        return self.weights[0] if self.weights is not None else None\n",
    "    \n",
    "    @property\n",
    "    def coef(self):\n",
    "        \"\"\"Return the coefficients (excluding bias).\"\"\"\n",
    "        return self.weights[1:] if self.weights is not None else None"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test the LinearRegression class\n",
    "model = LinearRegression(method='pinv')\n",
    "model.fit(X_raw, y_multi)\n",
    "\n",
    "print(\"Model trained successfully!\")\n",
    "print(f\"\\nBias (w₀): {model.bias:.4f}\")\n",
    "print(f\"Coefficients: {model.coef}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_class = model.predict(X_raw)\n",
    "\n",
    "# Evaluate\n",
    "r2_score = model.score(X_raw, y_multi)\n",
    "print(f\"\\nR² Score: {r2_score:.4f}\")\n",
    "\n",
    "# Test on new data\n",
    "X_test = np.array([[1.5, -2.0], [0.0, 0.0], [-1.0, 3.0]])\n",
    "y_test_pred = model.predict(X_test)\n",
    "print(f\"\\nPredictions on test data:\")\n",
    "for i, (x_val, pred) in enumerate(zip(X_test, y_test_pred)):\n",
    "    print(f\"  X = {x_val} → y = {pred:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Normal Equations**: Provide a closed-form solution to linear regression\n",
    "   $$\\mathbf{w} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}$$\n",
    "\n",
    "2. **Design Matrix**: Always include a column of ones for the bias term\n",
    "\n",
    "3. **Pseudo-Inverse**: More numerically stable alternative\n",
    "   $$\\mathbf{w} = \\mathbf{X}^+ \\mathbf{y}$$\n",
    "\n",
    "4. **SVD**: Most robust method, especially for ill-conditioned matrices\n",
    "\n",
    "5. **Choice of Method**:\n",
    "   - Small, well-conditioned problems: Normal equation\n",
    "   - Numerical stability needed: Pseudo-inverse or SVD\n",
    "   - Large datasets: Consider gradient descent instead\n",
    "\n",
    "### Advantages:\n",
    "- ✓ Exact solution (no hyperparameters)\n",
    "- ✓ No iterative optimization needed\n",
    "- ✓ Fast for small to medium problems\n",
    "\n",
    "### Disadvantages:\n",
    "- ✗ Computationally expensive for large $n$ ($O(n^3)$)\n",
    "- ✗ Requires matrix inversion (can be numerically unstable)\n",
    "- ✗ Not suitable for online learning\n",
    "\n",
    "### When to Use:\n",
    "- Number of features < 10,000\n",
    "- Full batch training\n",
    "- Need exact solution\n",
    "- Interpretability is important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Further Exploration\n",
    "\n",
    "Topics to explore next:\n",
    "- Regularization (Ridge, Lasso)\n",
    "- Gradient descent methods\n",
    "- Polynomial regression\n",
    "- Feature scaling and normalization\n",
    "- Cross-validation\n",
    "- Diagnostic plots (residuals, Q-Q plots)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
