{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio: Clasificaci√≥n de Lenguaje de Se√±as Americano (ASL)\n",
    "\n",
    "En este ejercicio, construir√°s una red neuronal para clasificar im√°genes del lenguaje de se√±as americano.\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "- Descargar y preparar un dataset desde Kaggle\n",
    "- Normalizar datos de im√°genes\n",
    "- Dise√±ar una arquitectura de red neuronal\n",
    "- Implementar m√©tricas de evaluaci√≥n\n",
    "- Entrenar y validar un modelo\n",
    "- Identificar problemas de overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 1: Obtener los Datos\n",
    "\n",
    "### Instrucciones:\n",
    "1. Ve a Kaggle y busca el dataset **\"Sign Language MNIST\"**\n",
    "2. Descarga los archivos:\n",
    "   - `sign_mnist_train.csv`\n",
    "   - `sign_mnist_test.csv`\n",
    "3. Crea una carpeta llamada `asl_data` en el mismo directorio que este notebook\n",
    "4. Coloca los archivos descargados dentro de la carpeta `asl_data`\n",
    "\n",
    "**Link del dataset:** https://www.kaggle.com/datasets/datamunge/sign-language-mnist\n",
    "\n",
    "Una vez descargados, ejecuta la siguiente celda para verificar:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 2: Importar Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 3: Cargar los Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datasets\n",
    "train_df = pd.read_csv(\"asl_data/sign_mnist_train.csv\")\n",
    "valid_df = pd.read_csv(\"asl_data/sign_mnist_test.csv\")\n",
    "\n",
    "print(f\"Datos de entrenamiento: {train_df.shape}\")\n",
    "print(f\"Datos de validaci√≥n: {valid_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar las etiquetas de las caracter√≠sticas\n",
    "# Completa aqu√≠ \n",
    "# Completa aqu√≠ \n",
    "# Completa aqu√≠ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 4: Explorar los Datos\n",
    "\n",
    "Antes de continuar, visualicemos algunas im√°genes del dataset para entender con qu√© estamos trabajando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'for' statement on line 5 (95844115.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 9\u001b[0;36m\u001b[0m\n\u001b[0;31m    plt.tight_layout()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'for' statement on line 5\n"
     ]
    }
   ],
   "source": [
    "# Visualizar algunas im√°genes de ejemplo\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(10):\n",
    "    # Reshape para formar una imagen 28x28\n",
    "    # Completa aqu√≠  \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Informaci√≥n sobre las clases\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClases √∫nicas en train:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28msorted\u001b[39m(\u001b[43my_train\u001b[49m\u001b[38;5;241m.\u001b[39munique()))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN√∫mero de clases √∫nicas:\u001b[39m\u001b[38;5;124m\"\u001b[39m, y_train\u001b[38;5;241m.\u001b[39mnunique())\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValor m√≠nimo:\u001b[39m\u001b[38;5;124m\"\u001b[39m, y_train\u001b[38;5;241m.\u001b[39mmin())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Informaci√≥n sobre las clases\n",
    "print(\"Clases √∫nicas en train:\", sorted(y_train.unique()))\n",
    "print(\"N√∫mero de clases √∫nicas:\", y_train.nunique())\n",
    "print(\"Valor m√≠nimo:\", y_train.min())\n",
    "print(\"Valor m√°ximo:\", y_train.max())\n",
    "\n",
    "# Distribuci√≥n de clases\n",
    "plt.figure(figsize=(12, 4))\n",
    "y_train.value_counts().sort_index().plot(kind='bar')\n",
    "plt.title('Distribuci√≥n de Clases en el Conjunto de Entrenamiento')\n",
    "plt.xlabel('Clase')\n",
    "plt.ylabel('Cantidad')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 5: Normalizaci√≥n de Datos\n",
    "\n",
    "### Normalizar los datos\n",
    "\n",
    "Los valores de los p√≠xeles est√°n en el rango [0, 255]. Para que la red neuronal entrene mejor, necesitamos normalizarlos al rango [0, 1].\n",
    "\n",
    "\n",
    "1. Verifica el rango actual de valores con `.min()` y `.max()`\n",
    "2. Normaliza `x_train` y `x_valid` dividiendo por 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_____' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# TODO: Verificar el rango de valores actual\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValor m√≠nimo antes de normalizar:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43m_____\u001b[49m)  \u001b[38;5;66;03m# Completa aqu√≠\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValor m√°ximo antes de normalizar:\u001b[39m\u001b[38;5;124m\"\u001b[39m, _____)  \u001b[38;5;66;03m# Completa aqu√≠\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# TODO: Normalizar los datos dividiendo por 255\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name '_____' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: Verificar el rango de valores actual\n",
    "print(\"Valor m√≠nimo antes de normalizar:\", _____)  # Completa aqu√≠\n",
    "print(\"Valor m√°ximo antes de normalizar:\", _____)  # Completa aqu√≠\n",
    "\n",
    "# TODO: Normalizar los datos dividiendo por 255\n",
    "x_train = _____  # Completa aqu√≠\n",
    "x_valid = _____  # Completa aqu√≠\n",
    "\n",
    "# Verificar la normalizaci√≥n\n",
    "print(\"\\nValor m√≠nimo despu√©s de normalizar:\", x_train.min())\n",
    "print(\"Valor m√°ximo despu√©s de normalizar:\", x_train.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 6: Crear el Dataset y DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mMyDataset\u001b[39;00m(\u001b[43mDataset\u001b[49m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_df, y_df):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(x_df)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x_df, y_df):\n",
    "        self.xs = torch.tensor(x_df).float().to(device)\n",
    "        self.ys = torch.tensor(y_df.values).to(device)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.xs[idx]\n",
    "        y = self.ys[idx]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_data = MyDataset(x_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_N = len(train_loader.dataset)\n",
    "\n",
    "valid_data = MyDataset(x_valid, y_valid)\n",
    "valid_loader = DataLoader(valid_data, batch_size=BATCH_SIZE)\n",
    "valid_N = len(valid_loader.dataset)\n",
    "\n",
    "print(f\"Tama√±o del conjunto de entrenamiento: {train_N}\")\n",
    "print(f\"Tama√±o del conjunto de validaci√≥n: {valid_N}\")\n",
    "print(f\"N√∫mero de batches de entrenamiento: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 7: Definir la Arquitectura del Modelo\n",
    "\n",
    "### Crear tu modelo\n",
    "\n",
    "Debes crear una red neuronal secuencial \n",
    "\n",
    "\n",
    "**Pistas:**\n",
    "- `input_size = 28 * 28`\n",
    "- `n_classes` debe ser el n√∫mero correcto para evitar el error \"out of bounds\"\n",
    "- Usa `nn.Sequential()` para encadenar las capas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define las constantes\n",
    "input_size = _____  # 28 * 28\n",
    "n_classes = _____   # Recuerda: y_train.max() + 1\n",
    "\n",
    "# TODO: Crea el modelo usando nn.Sequential\n",
    "model = nn.Sequential(\n",
    "\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 8: Definir Loss y Optimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 9: Funci√≥n de Accuracy\n",
    "\n",
    "### Implementar get_batch_accuracy\n",
    "\n",
    "Esta funci√≥n debe:\n",
    "1. Obtener las predicciones (clase con mayor probabilidad) usando `argmax`\n",
    "2. Comparar las predicciones con las etiquetas verdaderas\n",
    "3. Contar cu√°ntas son correctas\n",
    "4. Retornar la proporci√≥n de aciertos dividido por N (tama√±o total del dataset)\n",
    "\n",
    "**Pistas:**\n",
    "- `output.argmax(dim=1, keepdim=True)` obtiene la clase predicha\n",
    "- `pred.eq(y.view_as(pred))` compara predicciones con etiquetas\n",
    "- `.sum().item()` suma los aciertos\n",
    "- Retorna `correct / N`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Completa la funci√≥n get_batch_accuracy\n",
    "def get_batch_accuracy(output, y, N):\n",
    "    \"\"\"\n",
    "    Calcula la accuracy de un batch.\n",
    "    \n",
    "    Args:\n",
    "        output: Logits del modelo (sin softmax)\n",
    "        y: Etiquetas verdaderas\n",
    "        N: Tama√±o total del dataset\n",
    "    \n",
    "    Returns:\n",
    "        Proporci√≥n de aciertos en este batch respecto al dataset total\n",
    "    \"\"\"\n",
    "    pred = _____  # Obtener predicciones con argmax\n",
    "    correct = _____  # Contar cu√°ntas predicciones son correctas\n",
    "    return _____  # Retornar correct / N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 10: Funciones de Entrenamiento y Validaci√≥n\n",
    "\n",
    "Estas funciones ya est√°n implementadas para ti. Est√∫dialas cuidadosamente para entender el proceso de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \"\"\"\n",
    "    Entrena el modelo por una √©poca completa.\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    model.train()  # Modo entrenamiento\n",
    "    for x, y in train_loader:\n",
    "        # Forward pass\n",
    "        output = model(x)\n",
    "        \n",
    "        # Calcular loss\n",
    "        batch_loss = loss_function(output, y)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()  # Limpiar gradientes\n",
    "        batch_loss.backward()  # Calcular gradientes\n",
    "        optimizer.step()       # Actualizar pesos\n",
    "\n",
    "        # Acumular m√©tricas\n",
    "        loss += batch_loss.item()\n",
    "        accuracy += get_batch_accuracy(output, y, train_N)\n",
    "    \n",
    "    print('Train - Loss: {:.4f} Accuracy: {:.4f}'.format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate():\n",
    "    \"\"\"\n",
    "    Valida el modelo en el conjunto de validaci√≥n.\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "\n",
    "    model.eval()  # Modo evaluaci√≥n\n",
    "    with torch.no_grad():  # No calcular gradientes\n",
    "        for x, y in valid_loader:\n",
    "            output = model(x)\n",
    "\n",
    "            loss += loss_function(output, y).item()\n",
    "            accuracy += get_batch_accuracy(output, y, valid_N)\n",
    "    \n",
    "    print('Valid - Loss: {:.4f} Accuracy: {:.4f}'.format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 11: Entrenar el Modelo\n",
    "\n",
    "Ahora ejecutemos el entrenamiento por 20 √©pocas y observemos los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "    train()\n",
    "    validate()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para analizar y discutir\n",
    "\n",
    "Observa cuidadosamente los resultados del entrenamiento:\n",
    "- ¬øQu√© pasa con la accuracy de entrenamiento?\n",
    "- ¬øQu√© pasa con la accuracy de validaci√≥n?\n",
    "- ¬øLas dos siguen el mismo patr√≥n?\n",
    "\n",
    "**Pregunta:** Posiblemente obtengas que el accuracy de entrenamiento alcanz√≥ un nivel bastante alto, pero la accuracy de validaci√≥n no fue tan alta. ¬øQu√© sucedi√≥ aqu√≠?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Haz clic aqu√≠ para ver la explicaci√≥n</b></summary>\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Explicaci√≥n: Overfitting\n",
    "\n",
    "Este es un ejemplo de que el modelo est√° aprendiendo a categorizar los datos de entrenamiento, pero tiene un rendimiento pobre contra datos nuevos que no ha visto durante el entrenamiento. \n",
    "\n",
    "Esencialmente, el modelo est√° **memorizando** el dataset, pero no est√° ganando una comprensi√≥n robusta y general del problema. \n",
    "\n",
    "Este es un problema com√∫n llamado **overfitting** (sobreajuste).\n",
    "\n",
    "### Se√±ales de Overfitting:\n",
    "1. ‚úÖ Alta accuracy en entrenamiento (~95%+)\n",
    "2. ‚ùå Accuracy de validaci√≥n significativamente menor\n",
    "3. üìà La brecha entre train y valid aumenta con las √©pocas\n",
    "\n",
    "### ¬øPor qu√© sucede?\n",
    "- El modelo tiene demasiada capacidad (muchos par√°metros)\n",
    "- Aprende patrones espec√≠ficos del conjunto de entrenamiento\n",
    "- No generaliza bien a datos nuevos\n",
    "\n",
    "### Soluciones (que veremos en pr√≥ximas clases):\n",
    "- **Regularizaci√≥n**: L1, L2, Dropout\n",
    "- **Data Augmentation**: Aumentar variedad de datos\n",
    "- **Early Stopping**: Detener el entrenamiento cuando la validaci√≥n empeora\n",
    "- **Simplificar el modelo**: Menos capas o neuronas\n",
    "\n",
    "---\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BonusTrack \n",
    "\n",
    "1. **Visualizar predicciones incorrectas**: \n",
    "   - Encuentra im√°genes que el modelo clasific√≥ mal\n",
    "   - ¬øPuedes identificar patrones en los errores?\n",
    "\n",
    "2. **Experimentar con la arquitectura**:\n",
    "   - ¬øQu√© pasa si usas menos neuronas (256 en vez de 512)?\n",
    "   - ¬øQu√© pasa si agregas m√°s capas?\n",
    "\n",
    "3. **Graficar el learning curve**:\n",
    "   - Guarda la loss y accuracy de cada √©poca\n",
    "   - Grafica train vs valid para visualizar el overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
